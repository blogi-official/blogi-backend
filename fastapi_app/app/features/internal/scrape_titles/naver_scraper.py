import re
from datetime import datetime

from playwright.async_api import async_playwright

from app.features.internal.scrape_titles.config import CATEGORY_MAP


def clean_text(text: str) -> str:
    return re.sub(
        r"[\u200b-\u200f\u202a-\u202e\u2060-\u206f\ue000-\uf8ff]", "", text
    ).strip()


async def scrape_titles() -> list[dict]:
    result = []

    print("[DEBUG] Playwright ì‹œì‘ ì „")

    async with async_playwright() as p:
        print("[DEBUG] Playwright ì‹¤í–‰ë¨")
        browser = await p.chromium.launch(headless=True)
        print("[DEBUG] ë¸Œë¼ìš°ì € ì‹¤í–‰ë¨")
        page = await browser.new_page()
        print("[DEBUG] í˜ì´ì§€ ê°ì²´ ìƒì„±ë¨")

        for category, display_name in CATEGORY_MAP.items():
            # category_encoded = quote_plus(display_name)
            # query_encoded = quote_plus(query)

            category_encoded = display_name
            query = f"{display_name} ìˆí…ì¸ "
            print(query)

            query_encoded = query
            print(query_encoded)

            url = (
                f"https://search.naver.com/search.naver?"
                f"category={category_encoded}&query={query_encoded}"
                f"&sm=svc_clk.entnewsmore&ssc=tab.shortents.all"
            )
            print(f"[DEBUG] ì¹´í…Œê³ ë¦¬: {category}, URL: {url}")

            try:
                await page.goto(url, timeout=10000)
                print(f"[DEBUG] {category} í˜ì´ì§€ ì´ë™ ì™„ë£Œ")

                # TODO: ë§›ì§‘ë¶„ì•¼ í–ˆì§€ë§Œ ê¸°ì‚¬ë¡œ ë°”ê¾¸ê¸°. ì „ì²´ë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œë¡œ ìˆ˜ì • í˜¹ì€ ì¹´í…Œê³ ë¦¬ë³„ë¡œ mapë¡œ í•˜ê¸°
                titles = await page.eval_on_selector_all(
                    "span.sds-comps-text-type-headline2",
                    "elements => elements.map(el => el.innerText)",
                )

                print(f"[DEBUG] {category} ì œëª© ìˆ˜ì§‘ ì™„ë£Œ, ê°œìˆ˜: {len(titles)}")

                for t in titles:
                    result.append(
                        {
                            "title": clean_text(t),
                            "category": category,
                            "source": "ë„¤ì´ë²„",
                            "collected_at": datetime.utcnow().isoformat(),
                        }
                    )
                print(f"[ìŠ¤í¬ë˜í•‘ ê²°ê³¼] {category} ì¹´í…Œê³ ë¦¬ â†’ {len(titles)}ê°œ ìˆ˜ì§‘ë¨")

            except Exception as e:
                print(f"[ERROR] Failed to scrape {category}: {e}")

        await browser.close()
        print(f"[ì´ ìˆ˜ì§‘ ê²°ê³¼] ì´ í‚¤ì›Œë“œ {len(result)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    # ğŸ“Œ ì—¬ê¸°ì— ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ì¶”ê°€
    for item in result:
        print(f"[ìˆ˜ì§‘ëœ ì œëª©] {item['category']} - {item['title']}")

    return result
